\documentclass[../template]{subfiles}

\begin{document}
\section{Memorie}
\subsection{Distinzione delle memorie}
Una memoria è un'unità logica, dedicata alla memorizzazione dei dati nel calcolatore.
Le memorie sono classificate in base a 5 parametri:
\begin{itemize}
    \item Capacità: numero delle parole memorizzabili
        Chiamato $L$ il numero di linee, ed $N$ il numero di parole $L =\log_2 N$.
        Ogni parola è un dato ad $M$ bit.
        \\
        Dato che di solito $M = 8$, l'unità di misura la capacità di memoria si misura in byte.
    \item Caratteristiche fisiche:
        Di cui tipo (semiconduttore come RAM, a superficie magnetica HD, ottica DVD), il consumo,
        l'affidabilità misurata come MTBF (\textit{Mean Time Between Failure}), alterabilità (solo lettura o lettura/scrittura) e durevolezza (volatile o non-volatile)
    \item Organizzazione:
        memorie interne al calcolatore sono organizzate in una gerarchia,
        parallelismo ed interlacciamento (come si interfacciano con memorie di livello superiore)
    \item Modalità d'accesso: in quale modi si accede ai dati in memoria,
        \begin{itemize}
            \item Sequenziale: per accedere ad un dato in una posizione fissa è necessario leggere tutti i dati precedenti (cassette mangianastri)
            \item Diretto: è possibile accedere direttamente alla posizione in memoria, in molti casi però il tempo d'accesso è dipendente dalla posizione del dato rispetto alla posizione appena letta o scritta (cd)
            \item Casuale: un accesso diretto con tempo d'accesso costante (RAM, ROM)
            \item Associativo: si accedono agli indirizzi di memoria attraverso delle chiavi hash (tag), per controllare se un dato è presente vengono controllate tutte le locazioni esistenti.
        \end{itemize}

    \item Prestazioni:
        Per misurare le prestazioni di una memoria, esistono diversi parametri, uno di questi, come citato al punto precedente è il tempo d'accesso,  ovvero il tempo impiegato dalla memoria per raggiungere l'indirizzo da quando è fornito.

        Il tempo di ciclo $T_\text{rc}$ è definito come il tempo di accesso più il tempo necessario per terminare l'operazione prima di poter compiere un altro accesso.

        Mentre la velocità di trasferimento (bit rate) è l'inverso del tempo d'accesso, misurando il numero di bit trasferiti.

        Solitamente sono tutti dati imposti dalla CPU, a cui la memoria si adegua o introduce ritardi.

\end{itemize}

Il numero di pin in ingresso del componente della memoria non coincide necessariamente con $n_a$ ed $n_d$ del bus dei dati.
In un processore moderno, normalmente $n_a = 36$ quindi può indirizzare al massimo $64G$ di memoria RAM. Le dimensioni dell'unità di memoria può essere inferiore a $64G$, magari si hanno 4 memorie da $16G$, con $L=34$. $n_a$ rappresenta la massima memoria indirizzabile del processore, $L$ rappresenta la memoria effettiva di un cip specifico di memoria.

Le modalità d'accesso sono


\subsection{Gerarchie di memoria}
La gerarchia di memorie ha lo scopo di combinare tutte le caratteristiche migliori di ogni tipo di memoria per avvicinarsi il più possibile alla memoria ideale: capacità infinita, tempo d'accesso, costo e consumo nullo.

Salendo la gerarchia diminuisce il tempo d'accesso e la capacità delle memorie ma aumenta il costo per bit.

Al livello più alto della gerarchia sono presenti i registri interni alla CPU, segue la cache, poi la memoria centrale o principale, ed in fondo alla gerarchia le memorie secondarie, con alte capacità ma bassi costi e permanenti.
\subsubsection{Principio di località}

Un programma in un certo istante $t$ necessita di determinati dati
Se un programma ad un istante $t$ accede ad un certo dato,
c'è alta probabilità che siano richiesti anche i dati adiacenti (proprietà di località spaziale), e che all'istante successivo si acceda nuovamente alla memoria (proprietà di località temporale).

Gli algoritmi della MMU (\textit{Memory Management Unit}) si concentrano a minimizzare il numero di accessi in memoria a livelli più bassi.


\subsubsection{Blocco di memoria}
Con blocco si fa riferimento all'unità di informazione minima scambiata fra i livelli di memoria.
Quando un particolare dato viene richiesto ad una memoria, l'evento che il dato non sia presente prende il nome di "miss" mentre se il dato è già presente si chiama "hit".
\\
Lo scopo di una buona gerarchia di memoria è massimizzare il numero di hit. La frequenza con cui avviene una hit prende il nome di hit-rate ($h$), mentre la frequenza con cui avviene una miss è complementare
e viene chiamato miss rate.

Il tempo per recuperare il dato al livello inferiore in caso di miss prende il nome di "Miss Penalty", mentre se il dato è già disponibile, il tempo per recuperarlo è pari al tempo di hit.

Il miss penalty $T_\text{mp}$ può essere visto come la somma tra hit time ed il tempo richiesto per il trasferimento dei dati dai blocchi inferiori ($T_\text{miss}$).
\\
Ottenendo la relazione:
\[
    T_\text{acc} = h \cdot T_h + (1-h) T_\text{mp} = T_h + (1 -h) T_\text{miss}
\]
Dalla formula, minimizzando la miss rate ($1 -h$), si tende alla memoria ideale: $T_\text{acc} = T_h$.


\subsection{Gerarchie di memoria}
Una gerarchia di memoria è definita dai suoi livelli di memoria: il loro numero, dimensione, velocità e componenti.
\\
Il piazzamento o rimpiazzamento del blocco, chiamato anche funzione di traduzione o di mapping, sceglie dove allocare il blocco nel livello di memoria corrente.  Nel caso della cache ad esempio è necessario calcolare un hash per determinarne la locazione.

L'identificazione del blocco indica come risalire alla posizione del blocco di memoria.


I dati, una volta modificati, vengono scritti sui livelli inferiori attraverso delle strategie di scrittura.

Nel caso dei registri, l'identificazione è nominale, siccome il nome del registro è scritto nel codice sorgente,  mentre il rimpiazzamento è effettuato dal compilatore.

In memoria centrale, il piazzamento del blocco dipende dall'istruzione (specificato dall'indirizzo di memoria), i dati sono identificati dall'indirizzo e le scritture sono scelte a livello di codice.

Le memorie centrali nei calcolatori sono le RAM, volatili e di lettura e scrittura). Di memorie RAM ci sono due categorie:
SRAM (\textit{Static RAM}) utilizzate per memorie cache e DRAM (\textit{Dynamic RAM}) utilizzate come memoria centrale.

Le memorie SRAM sono molto veloci e realizzate attraverso flip flop D, che permettono di memorizzare il dato senza la perdita.
La tecnologia richiesta per realizzare uno di questi flip flop è molto costosa, per questo al giorno d'oggi non è possibile utilizzare queste memorie per immagazzinare grandi quantità di dati.

Oltre ai flip flop in queste memorie sono presenti logiche di indrizzo e la selezione di lettura scrittura.

Le memoria DRAM ogni bit è rappresentato da un condensatore / transistor (nel caso di tecnologia mos). Il loro costo è molto inferiore quindi rispetto alle precedenti.
\\
Siccome i condensatori col tempo si scaricano, necessitano di circuiti automatici di refresh per ricaricare il condensatore.

Oltre a memorie RAM sono presenti le memorie ROM, a sola lettura. Esse sono divise in:
le ROM originali erano scrivibili un'unica volta e non sono volatili.
PROM (\textit{Programmable ROM}), scrivibili un'unica volta,
EPROM (\textit{Erasable PROM}), cancellabili attraverso la luce ultravioletta,
EEPROM (\textit{Electrically Erasable PROM}), cancellabili con segnali elettrici.
FLASH leggibili e scrivibili con dati non volatili (come SSD ).


% Lez 2
\subsubsection{Memorie SRAM}
Le SRAM, memorie ram statiche, sono RAM accedute come se fossero una matrice di dati.
Normalmente il singolo dato è rappresentato da flip flop D.

Per studiare un ciclo di lettura di una memoria statica, prendiamo come esempio una ram con $N=1024$ e $M = 4bit$.
Il numero di bit necessari ad indirizzare i valori è quindi $L = \log_2 N = 10$.
La RAM ha quindi ingressi e 2 segnali di controllo CS (\textit{Chip Select}) per abilitare il dispositivo e WE (\textit{Write enable}).

Una volta caricato l'indirizzo nel bus degli indirizzi, si abilita CS e dopo un tempo $T_{acc}$ il dato viene reso disponibile dalla memoria sul bus dei dati.

Per scrittura viene abilitato il write enable e caricato il dato da scrivere nel bus dei dati, ed al momento dell'abilitazione di CS, dopo un certo tempo il dato viene scritto in memoria.

Al termine di ogni operazione CS torna a 0, disabilitando il chip.
\subsubsection{Memorie DRAM}
Nel caso di ram dinamiche, tipicamente con capacità maggiore es $8Gb$ il numero di bit necessari per indirizzare i dati dovrebbe essere $L=33$, portando un numero di ingressi esagerato per un chip di memoria.
\\
La soluzione per accedere a questo tipi di memoria è dividere la capacità come una matrice bidimensionale, es una matrice di $2K$ righe e $4M$ colonne, ottenendo numero di bit di indirizzamento pari alla dimensione maggiore.
\\
Quindi le DRAM sono più lente rispetto alle SRAM, anche per la necessità di avere un secondo indirizzo per identificare il dato n memoria.

I due segnali per identificare gli indirizzi di una DRAM prendono il nome di RAS (\textit{Row Address Strobe}) e CAS (\textit{Column Address Strobe}).

RAS viene utilizzato per l'ingresso di un decoder,
RAS viene dato in ingresso ad un decoder, identificando con 1 la riga da leggere/scrivere. In un secondo momento il segnale CAS viene utilizzato come ingresso di un demultiplexer, selezionando la colonna in ingresso del relativo indirizzo.

Nel caso di lettura il segnale passa attraverso un buffer DOUT, inviando i dati al processore, nel caso di scrittura i dati vengono letti dal processore scrivendo il dato nella riga e colonna selezionati.

Qua sotto se mi ricordo di aggiungerla sarà presente lo schema funzionale di una memoria DRAM.

Contatore di refresh e circuito di refresh si sostituiscono agli indirizzi di riga per leggere gli indirizzi di memoria e ricaricare i condensatori.
OE è equivalente a CS.

\subsubsection{Lettura e scrittura DRAM}
La differenza fondamentale è la necessità di inserire un secondo indirizzo per identificare i dati.
Prima si inserisce il valore di RAS nel bus dei dati, abilitandone la lettura con RAS. Successivamente si inserisce CAS abilitandone la lettura con CAS. Al momento dell'abilitazione di CAS, RAS non viene disabilitato, (spiegazione successiva).
\\
Successivamente dopo un tempo di accesso, nel caso di lettura viene fornito sul bus dei dati il valore contenuto in memoria.

Quando entrambi i valori RAS e CAS sono disabilitati l'output è in 3-state, quindi non più modificabile.

Per effettuare le write è necessario che il segnale WE sia abilitato prima di abilitare il CAS, altrimenti il valore contenuto in memoria verrebbe copiato nel bus.

\subsubsection{SDRAM}
Sono sempre delle DRAM, realizzate con condensatori, con la differenza che sono sincrone.
Infatti se si guarda lo schema logico di una DRAM, non è presente un segnale di clock, effettuando ogni operazione sul fronte di salita/discesa del CAS.

Il vantaggio principale di questo tipo di memoria è la certezza di avere il dato in memoria dopo un numero preciso di cicli di clock. Cosa non sempre vera nel caso di memoria asincrona, dove il tempo richiesto è variabile.

Lo svantaggio che portano è visibile nel caso in cui i dati siano disponibili prima che il ciclo di clock sia completato.
Questo è compensato dal \textbf{trasferimento a burst}.

La CPU è infatti in grado di scegliere il numero di dati da fare ritornare alla memoria. Nel caso di trasferimento singolo, quando la memoria è interrogata torna un solo indirizzo. In caso di trasferimento a burst la memoria restituisce ad ogni ciclo di clock, il dato all'indirizzo di memoria successivo a quello specificato.

Ad esempio nel caso di accesso ad un file, il primo valore viene letto dopo 4 cicli di clock, mentre i successivi direttamente al ciclo di clock seguente.
Questa lettura a bus è fondamentale per riempire la cache interna.
% Schema funzionale SDRAM

\subsubsection{DDR}
Le DDR (\textit{Double Data Rate}) nascono come un ulteriore miglioramento delle SDRAM. Come dice il nome, leggono due dati in un unico ciclo di clock, campionando sia sul fronte di salita che sul fronte di discesa.

Il principio di funzionamento è identico.
\subsubsection{FPM-DRAM}
\textit{Fast Page Mode DRAM} permettono di tenere il segnale di RAS\# attivo, variando solo il segnale CAS\# in modo da velocizzare tutti gli accessi successivi al valore in memoria, ottenendo trasferimenti di tipo 6-3-3-3 O 5-3-3-3.

\subsubsection{EDO-DRAM}
\textit{Extended Data Out DRAM}, permettono di risparmiare cicli di clock per trasferimenti successivi, perché l'uscita non richiedevano di disabilitare l'uscita (WE) ad ogni lettura.

\subsubsection{RDRAM}
\textit{Rambus DRAM} (Rambus è il nome di un consorzio), sviluppate da Intel.
Permettevano di avere chip con fino a 320 pin con velocità di trasferimento di $1.6Gb/s$, trasferimento a blocchi stile Fast.
Chip con pin su un unico lato per ridurre il consumo di energia.
Utilizzate in schede video ad alte prestazioni.

\subsection{Memorie Permanenti}
Sono memorie a semiconduttore, non volatili.
Le memorie FLASH (realizzate in tecnologia NAND o NOR) sono utilizzate per chiavette USB e SSD.
NVSRAM (\textit{Non Voltaile SRAM}) non sono molto diffuse per via dell'elevato costo di produzione.
\subsubsection{FeRAM}
Le \textit{Ferroelectirc RAM}, chiamate anche FRAM o F-RAM, rientrano nelle tecnologie di memorie non volatili emergenti, ed attraverso uno strato di materiale ferroelettrico, permettendo la permanenza ai dati anche in caso di mancanza di corrente.

Rispetto alle memorie flash consumano meno energia, hanno una maggiore velocità di scrittura ed un numero di scrittura/cancellazione maggiore.
\subsubsection{PCM}
Le \textit{Phase Change memory} sono composte da materiale in grado di cambiare fase (cristallina o amorfa). In particolare una regione amorfa presenta bassa riflettività e una regione cristallina presenta alta riflettività, permettendo attraverso punte o laser di leggerne il contenuto (funzionamento simile a CD/DVD).

\subsubsection{MRAM}
Le \textit{Magnetoresisteive RAM}, sfruttano l'effetto magnetoresistivo, non memorizzando le informazioni come quantità di carica elettrica (Come le RAM), ma come un campo magnetico.
\\
La lettura dell'informazione è ottenuta attraverso la misura della resistenza elettrica della cella.

\subsubsection{RRAM/CBRAM}
La \textit{Conductive Bridging RAM} è basata sulle proprietà di un elettrolita solido (tipicamente solfuro di germanio drogato con rame) che posto tra un elettrodo relativamente inerte (esempio tungsteno, materiale delle lampadine a filo) ed uno elettrochimicamente attivo  come argento o rame, fa si che quando si applica un campo elettrico viene provocato uno spostamento di ioni metallici nell'elettrolita, formando dei "nano-fili" conduttivi.

I principali vantaggi di questa tecnologia sono il basso consumo, l'elevata velocità di scrittura e la lunga durata.

\subsection{Interfaccia tra CPU e Memoria}
Tramite il bus degli indirizzi di dimensione \code{na}, unidirezionale verso la memoria, e tramite il bus dei dati di dimensione \code{nd} bidirezionale il processore scambia informazioni con la memoria.

Oltre a bus degli indirizzi e bus dei dati sono presenti bus di controllo, con segnali di read, write, \code{ads} (\textit{Address Strobe}) indica che il bus degli indirizzi è campionabile e presenta un'indirizzo valido (unidirezionale verso la memoria).
Nel caso di memorie sincrone, è presente anche \code{rdy} (\textit{Ready}) unidirezionale verso il processore, ed indica se il dato della memoria è pronto.

È presente anche un segnale M/IO  (\textit{Memory I/O}) che indica se si vuole accedere a memoria o input/output.
Infatti esistono due politiche per accedere alla memoria: le memory mapped IO, dove gli indirizzi di memoria sono considerati dal processore insieme agli indirizzi di input/output, accedere ad IO in questo caso vuole dire accedere alla memoria; nelle macchine intel come l'8086 gli indirizzi di memoria ed indirizzi per direttive ad input/output sono separati, quindi è necessario specificare se il dato nel bus degli indirizzi è destinato per la memoria o ad un dispositivo esterno.

\subsubsection{Architettura specifica 8086}
Sono presenti due ingressi di terra in modo da distribuire il carico
Il ciclo di lettura è diviso in quattro fasi, ed a fasi diverse, i 20 piedini per gli indirizzi possono essere considerati come ingressi per il bus degli indirizzi o ingressi per il bus dei dati. Questo comportamento viene indicato dicendo che il bus dei dati ed indirizzi è multiplexato.

NMI, INT e INTA servono per la gestione delle interruzioni. Il segnale d'ingresso INT indica la richiesta di interrupt al processore, il segnale INTA è il segnale in uscita del processore ed indica che il processore è in risposta all'interrupt.
Infatti il processore è in grado di ignorare gli interrupt nel caso stia effettuando calcoli importanti.
\\
Questo non è possibile se il segnale di interrupt che riceve in ingresso è NMI (\textit{Non Maskerable Interrupt}), dove il processore entro tempo prestabilito è obbligato a rispondere.

In uscita ha il segnale M/IO che indica se si sta lavorando con memoria o segnale IO.
Ha in ingresso il segnale di READY, proveniente dalla memoria.
Il segnale ALE (\textit{Address latch Enable}), equivalente all'AdS di prima. Segnale di Read e Write sono separati.

Segnale di test non utilizzato nel funzionamento normale, e i segnali di minmax specificavano la modalità di funzionamento dell'8086 (min modalità ridotta che consuma di meno).
Segnali di HOLD e HOLDA, sono utilizzati per permettere ad altri componenti di fornire i dati e controllare l'esecuzione, permettendo al processore di effettuare altri calcoli.

DTR \textit{Data transition / Receive}, dove quando è a 1 si ha un trasferimento dati da processore a memoria, e quando è a 0 il trasferimento è tra memoria e processore.

DEN \textit{Data Enable} indica che quello che sta leggendo in questo momento il processore sono dei dati. DEN e DTR funzionano sui transceiver bidirezionali, mentre l'ALE funziona sui latch.

BHE \textit{Bus High Enable} serve per l'interfacciamento con i chip fisici, perché il bus dei dati essendo a 16 bit e le memorie, avendo un parallelismo a 8 bit, per leggere due byte alla volta BHE permette di mettere due memorie in parallelo, leggendo nella parte alta dei 16 bit il valore dalla seconda memoria.

\subsubsection{Multiplexing nel dettaglio}
\begin{figure}[h]
    \centering
    \begin{circuitikz}
        \draw (0,0)
        -- (1, 0)
        -- ++(0, 1)
        -- ++(.5, 0)
        node[buffer, anchor=in, scale=0.5] (b) {}
        (b.out) -- ++(.5, 0)
        -- ++(0, -1) coordinate(x)
        -- ++(1, 0) coordinate(cout)
        (x) -- ++(0, -1)
        -- ++(-.5, 0)
        node[buffer, anchor=in, scale=.5, xscale=-1] (b2) {}
        (b2.out) -| (1, 0);

        \draw (cout) node[circ]{} node[above]{Out}
        (0, 0) node[circ]{} node[above]{In}
        (b2.out |- 0, 2) node[circ]{}
        node[right] {DT/R*}
        -- (b2.out |- 0, -.8)

        (b2.out |- 0, 1.8) -| (b.north)
        (b2.out |- 0, -.8) to[short, -o] (b2.north)
        ;
    \end{circuitikz}
    \caption{Transceiver}
\end{figure}

Nel transceiver Quando viene abilitato il segnale DTR, il segnale da input viene messo in output, diversamente il segnale passa da output ad input.

Nel caso di dati infatti, deve essere specificato in quale direzione i dati devono passare.
Nel transceiver Quando viene abilitato il segnale DTR, il segnale da input viene messo in output, diversamente il segnale passa da output ad input.

CS disabilita completamente il transceiver, non facendo passare il segnale

Dato che lo stesso numero di uscite della CPU deve guidare il bus dei dati ed il bus degli indirizzi, attraverso dei latch e dei transceiver bidirezionali, viene deciso dove instradare i segnali.

Nell istante T1, avremo quindi \code{DEN*=1} disattivando la scrittura del bus dei dati e \code{ALE = 1} campionando i bit nel bus degli indirizzi.
\\
In t3 \code{DEN*=0} abilitando i 3state, \code{ALE=0} indicando ai latch di non campionare (tenendo come valore presente nel bus degli indirizzi il valore precedente), \code{DTR = 0} abilitando scrittura nel 3state.

In T1, quindi si passa l'indirizzo di memoria, in T2 è una fase di bufferizzazione, dando il tempo alla memoria di campionare il buffer degli indirizzi, in T3 abilita il buffer dei dati ed in T4 è il ciclo di completamento.

\subsubsection{Lettura e Scrittura}
In fase di lettura, l'indirizzo di memoria è contenuto inizialmente in tutti e 20 i bit. Il segnale di \code{ALE} viene abilitato, permettendo al flip flop D di campionare i risultati, il data enable non è attivo.
\\
Il \code{DTR} rimane tutto il tempo a 0, siccome si tratta di una fase di lettura.

In fase T2 si porta il valore di \code{DEN} a 0, indicando la fine della fase in cui il bus multiplexato ha la funzione di indirizzo, abilito anche \code{RD*=0}

In fase T3 vengono caricati i valori dei 16 bit del dato di memoria nel bus degli indirizzi, essendo abilitato il valore di \code{RD} alla fase precedente e \code{DEN}.

Nella parte finale il \code{DTR} viene portato ad 1, abilitando la scrittura da processore a memoria.
\newpage
\subsection{Bus sincroni ed asincroni}
Le fasi T1, T2, T3 e T4 sono scandite dai diversi cicli di clock.
Il vantaggio di un'architettura sincrona è la certezza di sapere lo stato in cui mi trovo al momento, e la certezza dei tempi di esecuzione. L'alternativa è una struttura asincrona, gestita tramite un'protocollo che indica quando inizia e termina ogni fase.

Il metodo più comune per gestire approcci asincroni, è attraverso segnali di handshake:
ARDY (\textit{Address Ready}),equivalente ad ADS e ALE, e DRDY sono i segnali utilizzati dal protocollo di handshake.
Il segnale ARDY indica di iniziare la fase di indirizzo.
Sono presenti anche i segnali di acknowledge, i quali dal nome riconoscono di aver ricevuto il segnale.

Prendendo come esempio il caso di un ciclo di lettura, il processore manda gli indirizzi alla memoria con ARDY. La memoria dopo un certo tempo la memoria risponde con un segnale di acknowledge \code{ack}.
Il processore ricevendo il segnale \code{ack} capisce che la memoria ha effettuato l'elaborazione dell'indirizzo e disabilita il segnale ADRY, comunicando alla memoria che ha ricevuto l'acknowledgement.
Quando la memoria ha copiato tutti i dati, porta anch'essa il segnale \code{ack=0}.

Si esegue un'analogo procedimento per il trasferimento dei dati: il processore attende il segnale \code{drdy} dalla memoria, il quale indica quando i dati letti sono presenti sul bus. Appena riceve il segnale risponde con \code{ack} indicando alla memoria che ha ricevuto i dati.
\\
Una volta ricevuti questi dati, la memoria toglie il segnale \code{drdy}.
\subsubsection{Interfaccia I/o}
Per i dispositivi di input/output è utilizzata comunque la logica sincrona. Dove il campionamento è effettuato sempre attraverso flip-flop.
% Una logica analoga vale per la comunicazione con i dispositivi di input/output

\subsection{Indirizzamento e accesso in moduli}
Quello visto fino ad ora è l'interfacciamento tra processore e memoria logica, un'unica entità con un bus degli indirizzi ed un bus di dati con dimensioni pari a quelle del sistema.

Nel caso le memorie a disposizione abbiano una dimensione inferiore a quella del bus di dati ($L < n_a$), si può escludere gli ultimi $k$ bit dell'indirizzo meno significativi, (con $k = n_a - L$), quindi per leggere un'elemento di 32 bit all'indirizzo 100, con a disposizione quattro memorie in parallelo, ottengo il dato leggendolo da un'unica riga, indicata dai bit più significativi rimanenti: ovvero 1000 1001 1010 1011 se $k = 2$ la riga è 10.
\\
Il problema nasce quando è necessario leggere una parola di 32 bit all'indirizzo 1001, dove la cella degli ultimi 8 bit sarebbe 1100, indicando una riga differente. È necessario effettuare la lettura con accessi in memoria differenza.

Ricordiamo che al capitolo sulle architetture risc e cisc (pg. ???) è stato detto che le architetture RISC preferiscono indirizzi di memoria allineati, garantendo letture più veloci pur consumando spazio in memoria.

Nel caso di letture da righe differenti, per realizzare tale modello di memoria, sono necessari CS separati, in modo da abilitare e disabilitare la scrittura sul bus delle celle con righe differenti. I segnali di CS vengono chiamati di byte enable.

In caso di parole allineate, l'accesso è possibile in un unico ciclo, abilitando il valore dei byte enable a tutte le memorie.

Nel caso dell'8086, è presente un solo piedino di byte enable, utilizzando \code{a0} come abilitazione delle memorie inferiori e \code{BHE} come abilitatore delle memorie superiori.

% Esercizi
\end{document}
