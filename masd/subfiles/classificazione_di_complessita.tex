\documentclass[../template]{subfiles}

\begin{document}
\section{Classificazione di complessità}
\subsection{Problemi di ottimizzazione}
Un problema di ottimizzazione è definito su un insieme di istanze $(f, S)$: funzione obbiettivo e regione ammissibile del problema.
\\
La forma normale di un problema di ottimizzazione di massimo è espressa come:
\[
    \max_{x \in S} f(x)
\]
Risolvere l'istanza $(f, S)$, vuol dire trovare $x' \in S$ tale per cui $f(x') \ge fx) \, \forall x \in S$. Il valore $x'$ viene detto soluzione ottima dell'istanza, mentre $f(x')$ prende il nome di valore ottimo.

Per risolvere il problema di ottimizzazione si richiede di trovare un algoritmo $\mathcal{A}$, in grado di determinare il valore ottimo su regione ammissibile $S$. Il numero di operazioni eseguite prende viene indicato com $numop_\mathcal{A}(I)$.
\\
Oltre al numero di operazioni, è calcolata anche la quantità di memoria necessaria all'algoritmo per processare l'istanza. Questa prende il nome di dimensione dell' istanza ed è indicata con $dim(I)$.

L'analisi worst case definisce il tempo $t_\mathcal{A}(k)$ necessario all'algoritmo $\mathcal{A}$ per risolvere istanze di dimensione $k$, come il massimo tra tutti i tempi di esecuzione:
\[
    t_\mathcal{A}(k) = \max_{I: dim(I) = k} numop_\mathcal{A}(I)
\]
Solitamente viene calcolata l'espressione esatta di $t_\mathcal{A}(k)$, ma ci si limita ad uno studio dell'ordine di grandezza.
Il valore $t_\mathcal{A}(k)$ è detto dell'ordine di grandezza di $g(k)$, se esiste $u$ costante positiva tale per cui $t_\mathcal{A}(k) \le u \cdot g(k)$.

Un problema di ottimizzazione può essere classificato in diverse categorie in base alle caratteristiche dell'insieme $S$:
se $S$ è continuo, allora il problema è detto di ottimizzazione continua; Diversamente se $S$ è discreto, ovvero i suoi elementi sono enumerabili, si parla di problema ad ottimizzazione combinatoria.


% Clique (ricerca di sottografi con almeno un arco)
% TSP (\textit{Traveling Salesman Problem}): dato un grafo completo $G=(V, A)$ con distanze degli archi $d_{ij}$ interi non negativi.
% Individuare nel grafo il circuito hemiltoniano di cammino minimo.

Per i problemi di ottimizzazione combinatoria, è semplice identificare un algoritmo risolutivo, infatti basta effettuare un'enumerazione completa per elencare tutte le possibili soluzioni.

Enumerare tutte le soluzioni di un problema può richiedere tempi molto elevati, per questo è utile sapere in anticipo se è possible trovare soluzioni allo stesso problema eseguibili in tempo ragionevole.

\subsection{Complessità}

Algoritmi con tempi di esecuzione dell'ordine di grandezza $2^k$ o $k!$ sono di complessità esponenziale. In essi il tempo di esecuzione cresce molto rapidamente con l'aumento delle istruzioni dell'algoritmo.
Diversamente, algoritmi con tempi dell'ordine di grandezza di $k^p$, con $p$ costante, vengono detti di complessità polinomiale.
\\
Maggiore è l fattore $p$ del polinomio, maggiore è il tempo di esecuzione. Rimane comunque vero che la complessità polinomiale è preferibile rispetto a quella esponenziale.


\def\np{\ensuremath{\mathcal{NP}\,}}
\def\p{\ensuremath{\mathcal{P}\,}}

Dato un problema di ottimizzazione $R$, diciamo che esso appartiene alla classe \p se e solo se esiste un algoritmo $\mathcal{A}$ di complessità polinomiale in grado di risolverlo.
\\
A questa classe appartengono problemi come shortest path ed MST.

Un problema è detto di classe \np se la soluzione ottima non è calcolabile in tempo polinomiale.
La classe di problemi \np contiene tutti i problemi di ottimizzazione, è possibile verificare la soluzione in tempo polinomiale.
\\
Dentro a questa categoria è presente la Clique, il cui valore ottimo si ottiene calcolandone la cardinalità, ed il TSP, il cui valore ottimo è calcolabile come somma delle distanze.

Va notato che tutti i problemi appartenenti alla classe \p, appartengono anche alla classe \np, infatti non è neanche necessario fornire la soluzione ottima per calcolarne il valore in tempo polinomiale.

\subsubsection{Trasformazione in tempo polinomiale}
Dati due problemi di ottimizzazione $R_1$ ed $R_2$, diciamo che $R_1$ è trasformabile in tempo polinomiale in $R_2$ se e solo se ogni istanza di $R_1$ di dimensione $k$ può essere risolta risolvendo l'istanza $R_2$ di dimensione al più $p(k)$ per qualche polinomio $p$.
Se $R_1$ è trasformabile in tempo polinomiale in $R_2$, allora anche $R_1$ è risolvibile in tempo polinomiale.

\subsubsection{Dimostrazione}
Sia $k$ la dimensione di un'istanza di problema $R_1$ e $p(k)$ la dimensione dell'istanza equivalente $R_2$.
\\
Ogni istanza di $R_2$ di dimensione $h$ viene risolta in un tempo polinomiale $q(h)$.
\\
Segue che per risolvere l'istanza $R_2$ sarà necessario un tempo $q(p(k))$, polinomiale rispetto a $k$.

\subsubsection{Problemi NP-completi}
Diciamo che un problema $R$ è NP-completo se $R \in \mathcal{NP}$ e per ogni problema $Q \in \mathcal{NP}$ esiste una riduzione polinomiale $Q$ in $R$.

Se esistesse un problema NP-completo risolvibile in tempo polinomiale, allora si avrebbe che $\mathcal{P} = \mathcal{NP}$

Avendo visto che $\mathcal{P} \subseteq \mathcal{NP}$, possiamo chiederci se esistono problemi in $\mathcal{NP}$ non risolvibili in tempo polinomiale: $\mathcal{P} \neq \mathcal{NP}$.

Sino ad ora questa domanda non ha ancora una risposta, tuttavia si presuppone che probabilmente $\mathcal{P} \neq \mathcal{NP}$.

\subsection{Problemi di approssimazione}
Dato un problema di ottimizzazione di istanze $(f, S)$ tali che $\forall x \in S, \quad f(x) \ge 0$, chiamata $x'$ la soluzione ottima dell'istanza, e $v = f(x')$ il valore ottimo dell'istanza:

Per i problemi di massimo, il problema di $\varepsilon$-approssimazione (epsilon-approssimazione), $\varepsilon \ge 0$, consiste nel determinare un punto $\bar{x} \in S$ tale che
\[
    \frac{v}{f(\bar{x})} \le 1 + \varepsilon
\]
Il punto $\bar{x}$ prende il nome di soluzione $\varepsilon$-approssimata  dell'istanza.

Per valori di $\varepsilon =0$, il problema coincide con quello di ottimizzazione, ma per $\varepsilon > 0$ si richiede una soluzione che si discosti al massimo di una percentuale $\varepsilon$ del valore $f(\bar{x})$. Quindi tanto maggiore il valore di $\varepsilon$ quanto minore sarà la precisione garantita da una soluzione.

\subsubsection{Algoritmi di approssimazione}
Un algoritmo $\mathcal{A}_\varepsilon$ si definisce algoritmo di $\varepsilon$-approssimazione per un problema di ottimizzazione, se risolve il problema di $\varepsilon$-approssimazione associato ad ogni istanza del problema di ottimizzazione.


Dato un problema $\mathcal{NP}$-completo, la complessità del corrispondente problema di $\varepsilon$-approssimazione dipende dalla categoria del problema di approssimazione:
\begin{enumerate}
    \item Per ogni valore di $\varepsilon > 0$ il problema è risolvibile in tempo polinomiale sia per dimensione delle istanze, sia rispetto all'inverso della precisione richiesta.
    \item Per ogni valore di $\varepsilon > 0$ il problema richiede tempo polinomiale rispetto alla dimensione delle istanze, ma esponenziale rispetto a $\frac{1}{\varepsilon}$
    \item Il problema è risolvibile in tempo polinomiale solo per valori alti di $\varepsilon$
    \item Per ogni valore di $\varepsilon$ il problema è $\mathcal{NP}$-completo.
\end{enumerate}
\end{document}
